---
title: "Линейная регрессия"
subtitle: "Линейные модели..."
author: "Вадим Хайтов, Марина Варфоломеева"
output:
  ioslides_presentation:
    css: assets/my_styles.css
    logo: assets/Linmod_logo.png
    widescreen: yes
---


## Мы рассмотрим 

- Базовые идеи корреляционного анализа
- Проблему двух статистических подходов: "Тестирование гипотез vs. построение моделей"
- Разнообразие статистических моделей
- Основы регрессионного анализа

### Вы сможете

+ Оценить взаимосвязь между измеренными величинами
+ Объяснить что такое линейная модель
+ Формализовать запись модели в виде уравнения
+ Подобрать модель линейной регрессии
+ Протестировать гипотезы о наличии зависимости при помощи t-критерия или F-критерия
+ Оценить предсказательную силу модели 

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

# Знакомимся с даными

## Пример: IQ и размеры мозга
Зависит ли уровень интеллекта от размера головного мозга? (Willerman et al. 1991)

<div class="columns-2"> 

![Scan_03_11](images/MRI-Scan_03_11-by_bucaorg_Paul_Burnett_no_Flickr.jpg)
<small>[Scan_03_11](https://flic.kr/p/c45eZ3) by bucaorg(Paul_Burnett) on Flickr</small>  

<br/>

Было исследовано 20 девушек и 20 молодых людей

У каждого индивида измеряли:

- вес, 
- рост, 
- размер головного мозга (количество пикселей на изображении ЯМР сканера)
- Уровень интеллекта измеряли с помощью IQ тестов

Пример взят из работы: Willerman, L., Schultz, R., Rutledge, J. N., and Bigler, E. (1991), "In Vivo Brain Size and Intelligence," Intelligence, 15, 223-228.  
Данные представлены в библиотеке *"The Data and Story Library"* 
http://lib.stat.cmu.edu/DASL/  

</div>

## Знакомство с данными

Посмотрим на датасет

```{r, echo=FALSE}
# library(downloader)
# download("https://varmara.github.io/linmodr/data/IQ_brain.csv", destfile = "data/IQ_brain.csv" )
```

```{r}
brain <- read.csv("data/IQ_brain.csv", header = TRUE)
head(brain)

```

Есть ли пропущенные значения?

```{r}
sum(!complete.cases(brain))
```


## Где пропущенные значения?

Где именно?

```{r}
sapply(brain, function(x) sum(is.na(x)))
```

Что это за случаи?

```{r}
brain[!complete.cases(brain), ]
```

Каков объем выборки

```{r}
nrow(brain) ## Это без учета пропущенных значений
```


# Корреляционный анализ

*Цель практически любого исследования* - поиск взаимосвязи величин и создание базы для предсказания неизвестного на основе имеющихся данных

## Корреляционный анализ

Наличие связи между явлениями __не означает__, что между ними существует причинно-следственная связь.    

Сила связи между явлениями может быть количественно измерена.

<!-- ## Связь между уровнем интеллекта и размером головного мозга  -->

<!-- ```{r fig.width=3.75, fig.height=2.25, out.width='2.5in', out.height='1.5in'} -->
<!-- library (ggplot2) -->
<!-- theme_set(theme_bw()) -->
<!-- pl_brain <- ggplot(data = brain, aes(x = MRINACount, y = PIQ)) +  -->
<!--   geom_point() +  -->
<!--   labs(x = 'Размер мозга (MRINACount)', y = 'Уровень интеллекта (PIQ)') -->
<!-- pl_brain -->
<!-- ``` -->


## Основные типы линейной связи между величинами


```{r, echo = FALSE, purl=FALSE}
library (gridExtra)

x <- rnorm(100, 10, 5)
y1 <- 5*x + rnorm(100, 0, 5)
pl_pos_cor <- ggplot(data.frame(x = x, y = y1), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина') + 
  ggtitle('Положительная \nкорреляция') 

y2 <- -5*x + rnorm(100, 0, 5)
pl_neg_cor <- ggplot(data.frame(x = x, y = y2), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина') + 
  ggtitle('Отрицательная \nкорреляция') 

y3 <- 0*x + rnorm(100, 0, 5)
pl_zero_cor <- ggplot(data.frame(x = x, y = y3), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина') + 
  ggtitle('Корреляции \nнет') 

grid.arrange(pl_pos_cor, pl_neg_cor, pl_zero_cor, ncol = 3)

```

## Криволинейные связи между величинами

```{r, echo=FALSE, purl=FALSE}
x <- rep(seq(1, 20, 1), 5)
y1 <- 10*sin(x) + rnorm(100, 0, 5)
pl_sin_cor <- ggplot(data.frame(x = x, y = y1), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина')

y2 <- exp(x/3) + rnorm(100, 0, 50)
pl_exp_cor <- ggplot(data.frame(x = x, y = y2), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина') 

y3 <- exp(1 + -1*x)/(1 + exp(1 + -1*x)) + rnorm(100, 0, 0.05)
pl_log_cor <- ggplot(data.frame(x = x, y = y3), aes(x = x, y = y)) + 
  geom_point(colour = 'steelblue') + 
  xlab('Первая величина') + ylab('Вторая величина')

grid.arrange(pl_sin_cor, pl_exp_cor, ncol = 2)


```

## Коэффициент ковариации

Оценивает \textbf{сонаправленность отклонений} двух величин от своих средних значений
$$cov_{x,y} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{n - 1}$$


Коэффициент ковариации варьирует в интервале $-\infty < cov_{x,y} < +\infty$


```{r, echo=FALSE, purl=FALSE}
X_back <- rnorm(20, 0, 4)
Y_back <- rnorm(20, 1, 4)

XY <- data.frame(x = X_back, y = Y_back)

meanX <- 0
meanY <- 1
Xmin <- -5
Xmax <- 5
Ymin <- -5
Ymax <- 5

X <- 2
Y <- 4
X1 <- -2
Y1 <- -4
X2 <- 2
Y2 <- -4

ar <- arrow(type = 'closed', length = unit(0.15,'cm'))

Pl_positiv1 <- 
  ggplot() + 
  geom_point(data = XY, aes(x = x, y = y), color = "gray") + 
  geom_point(aes(x = c(meanX, X), y = c(meanY, Y))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = 'none') + 
  geom_segment(aes(x = meanX, y = meanY, xend = X - 0.2, yend = Y - 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X, y = Y, xend = Xmin, yend = Y), linetype = 2) +
  geom_segment(aes(x = X, y = Y, xend = X, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X, yend = Ymin), arrow = ar, color ="red", size = 1) + 
  labs(x = 'x', y = 'y') +
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y), arrow = ar, color = "red", size = 1) + 
  ggtitle('Положительные\nотклонения') 


Pl_positiv2 <- 
  ggplot() + 
  geom_point(data = XY, aes(x = x, y = y), color = "gray") + 
  geom_point(aes(x = c(meanX, X1), y = c(meanY, Y1))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = 'none') + 
  geom_segment(aes(x = meanX, y = meanY, xend = X1 + 0.2, yend = Y1 + 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X1, y = Y1, xend = Xmin, yend = Y1), linetype = 2) +
  geom_segment(aes(x = X1, y = Y1, xend = X1, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X1, yend = Ymin), arrow = ar, color = "blue", size = 1) + 
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y1), arrow = ar, color = "blue", size = 1) + 
  labs(x = 'x', y = 'y') +
  ggtitle('Отрицательные\nотклонения') 

Pl_negative <- 
  ggplot() + 
  geom_point(data = XY, aes(x = x, y = y), color = "gray") + 
  geom_point(aes(x = c(meanX, X2), y = c(meanY, Y2))) +
  xlim(Xmin, Xmax) + ylim(Ymin, Ymax) + guides(size = 'none') + 
  geom_segment(aes(x = meanX, y = meanY, xend = X2 - 0.2, yend = Y2 + 0.2), arrow = ar) +
  geom_segment(aes(x = meanX, y = meanY, xend = meanX, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = meanY, xend = Xmin, yend = meanY), linetype = 2) +
  geom_segment(aes(x = X2, y = Y2, xend = Xmin, yend = Y2), linetype = 2) +
  geom_segment(aes(x = X2, y = Y2, xend = X, yend = Ymin), linetype = 2) +
  geom_segment(aes(x = meanX, y = Ymin, xend = X2, yend = Ymin), arrow = ar, color = "red", size = 1)+ 
  geom_segment(aes(x = Xmin, y = meanY, xend = Xmin, yend = Y2), arrow = ar, color = "blue", size = 1) + 
  labs(x = 'x', y = 'y') +
  ggtitle('Отклонения\nв разных направлениях') 
```


```{r echo=FALSE, purl=FALSE, fig.height=2.25}
grid.arrange(Pl_positiv1, Pl_positiv2, Pl_negative, ncol = 3)
```



## Коэффициент корреляции



Это стандартизованное значение ковариации

$$r_{x,y} = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})} {\sqrt{\sum(x_i-\bar{x})^2}\sqrt{\sum(y_i-\bar{y})^2}} = \frac{cov_{x,y}} {\sigma_x \sigma_y}$$

Коэффициент корреляции варьирует в\ интервале $-1 \le r_{x,y} \le +1$


## Коэффициенты корреляции и условия их применимости   

Коэффициент | Функция | Особенности применения
|-------------|--------------------------------|-------------|
Коэф. Пирсона | `cor(x,y,method="pearson")` | Оценивает связь двух нормально распределенных величин. Выявляет только линейную составляющую взаимосвязи.
Ранговые коэффициенты (коэф. Спирмена, Кендалла) | `cor(x,y,method="spearman")`<br>`cor(x,y,method="kendall")`   | Не зависят от формы распределения. Могут оценивать связь для любых монотонных зависимостей. 


## Оценка статистической значимости коэффициентов корреляции

- Коэффициент корреляции - это статистика, значение которой описывает степень взаимосвязи двух сопряженных переменных. Следовательно применима логика статистического критерия. 
- Нулевая гипотеза $H_0: r=0$
- Бывают двусторонние $H_a: r\ne 0$ и односторонние критерии $H_a: r>0$ или $H_a: r<0$
- Ошибка коэффициента Пирсона: $SE_r=\sqrt{\frac{1-r^2}{n-2}}$
- Стандартизованная величина $t=\frac{r}{SE_r}$ подчиняется распределению Стьюдента с параметром $df = n-2$
- Для ранговых коэффициентов существует проблема "совпадающих рангов" (tied ranks), что приводит к приблизительной оценке $r$ и приблизительной оценке уровня значимости. 
- Значимость коэффициента корреляции можно оценить пермутационным методом


## Задание

+ Постройте точечную диаграмму, отражающую взаимосвязь между результатами IQ-теста (PIQ) и размером головного мозга (MRINACount)
+ Определите силу и направление связи между этими величинами
+ Оцените сатистическую значимость коэффициента корреляции Пирсона между этими двумя переменными 
+ Придумайте способ, как оценить корреляцию между всеми парами исследованных признаков

*Hint 1*: Обратите внимание на то, что в датафрейме есть пропущенные значения. Изучите, как работают с `NA` функции, вычисляющие коэффициенты корреляции. 

<!-- *Hint 2* Для построения точечной диаграммы вам понадобится `geom_point()` -->



## Решение

```{r pl-brain_expose, fig.width=4}
pl_brain <- ggplot(brain, 
               aes(x = MRINACount, y = PIQ)) + 
  geom_point() + 
  xlab("Brain size") + 
  ylab("IQ test")
pl_brain
```


## Решение
```{r}
cor.test(brain$PIQ, brain$MRINACount, method = "pearson", alternative = "two.sided")

```



## Решение 

```{r, size=2, tidy=TRUE}
cor(brain[,2:6], use = "pairwise.complete.obs")

```



<!-- ## Решение -->

<!-- ```{r, fig.align='center', fig.width=4} -->
<!-- pl_brain + theme_dark() +  -->
<!--   geom_point(aes(color = Gender), size = 4) +  -->
<!--   scale_color_manual(values =  c("red", "blue")) -->
<!-- ``` -->

##   Два подхода к исследованию: <br> Тестирование гипотезы <br>VS<br> Построение модели 

+ Проведя корреляционный анализ, мы лишь ответили на вопрос "Существует ли статистически значимая связь между величинами?"

+ Сможем ли мы, используя это знание, _предсказть_ значения одной величины, исходя из знаний другой? 

## Тестирование гипотезы VS построение модели 

- Простейший пример  
- Между путем, пройденным автомобилем, и временем, проведенным в движении, несомненно есть связь. Хватает ли нам этого знания?   
- Для расчета величины пути в зависимости от времени необходимо построить модель: $S=Vt$, где $S$ - зависимая величина, $t$ - независимая переменная, $V$ - параметр модели.
- Зная параметр модели (скорость) и значение независимой переменной (время), мы можем рассчитать (*cмоделировать*) величину пройденного пути


# Какие бывают модели?

## Линейные и нелинейные модели
<br>

Линейные модели 
$$y = b_0 + b_1x$$ <br> $$y = b_0 + b_1x_1 + b_2x_2$$ 
Нелинейные модели 
$$y = b_0 + b_1^x$$ <br>  $$y = b_0^{b_1x_1+b_2x_2}$$ 

## Простые и многокомпонентные (множественные) модели

+ Простая модель
 $$y = b_0 + b_1x$$ 

+ Множественная модель
 $$y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_nx_n$$ 


## Детерминистские и стохастические модели

<div class="columns-2">
```{r,echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2) + ylim(0, 100) 
```
Модель: $у_i = 2 + 5x_i$    
Два параметра: угловой коэффициент (slope) $b_1=5$; свободный член (intercept) $b_0=2$   
Чему равен $y$ при $x=10$?


```{r,echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x + rnorm(20,0, 20)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2)  + ylim(0,100) + theme_bw()

```
Модель: $у_i = 2 + 5x_i + \varepsilon_i$    
Появляется дополнительный член $\varepsilon_i$ 
Он вводит в модель влияние неучтенных моделью факторов. 
Обычно считают, что $\epsilon \in N(0, \sigma^2)$ 

</div>

## Случайная и фиксированая часть модели
В стохастические модели выделяется две части:

**Фиксированная часть:** $у_i = 2 + 5x_i$   
**Случайная часть:** $\varepsilon_i$ 

Бывают модели, в которых случайная часть выглядит существенно сложнее (модели со смешанными эффектами). В таких моделях необходимо смоделировать еще и поведение случайной части.




## Модели с дискретными предикторами

```{r , echo=FALSE}
set.seed(1234)
x <- data.frame(labels = c(rep("Level 1", 10), rep( "Level 2", 10), rep("Level 3", 10)), response = c(rnorm(10, 5, 1), rnorm(10, 10, 1), rnorm(10, 15, 1))) 

ggplot(x, aes(x=labels, y=response)) + geom_boxplot()+ geom_point(color="blue", size=4) + xlab(" ") 

```

Модель для данного примера имеет такой вид  
<br>
<br>
$response = 4.6 + 5.3I_{Level2} + 9.9 I_{Level3}$

$I_{i}$ - dummy variable   


## Модель для зависимости величины IQ от размера головного мозга

Какая из линий "лучше" описывает облако точек?

```{r, echo=FALSE, fig.align='center', fig.height= 5, fig.width=7}
library(ggplot2)

pl_1 <- pl_brain + geom_smooth(method = "lm", se = FALSE, size=2) + geom_abline(slope = 0.00008, intercept = 35, color="green", size = 2) + geom_abline(slope = 0.00014, intercept =1.7, color="red", size=2) 

grid.arrange (pl_brain, pl_1, ncol=2)

```




# Найти оптимальную модель позволяет регрессионный анализ
<div  align="right">

"Essentially, all models are wrong,     
but some are useful"     
(Georg E. P. Box) 

</div>

## Происхождение термина "регрессия"

<div class="columns-2">


<img src="images/Galton.png" width="220" height="299" >


Френсис Галтон (Francis Galton)


"the Stature of the adult offspring … [is] … more mediocre than the
stature of their Parents" (цит. по `Legendre & Legendre, 1998`)

Рост _регрессирует_ (возвращается) к популяционной средней     
Угловой коэффициент в зависимости роста потомков от роста родителей- _коэффциент регресси_


</div>



## Подбор линии регрессии проводится с помощью двух методов 

>- С помощью метода наименьших квадратов (Ordinary Least Squares) - используется для простых линейных моделей
<br>

>- Через подбор функции максимального правдоподобия (Maximum Likelihood) - используется для подгонки сложных линейных и нелинейных моделей.



## Кратко о методе макcимального правдоподобия {.columns-2 .smaller}


```{r gg-norm-tunnel, echo=FALSE, fig.height=5, fig.width=6, fig.align='left',  purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html

n <- 2


xy <- data.frame(X = rep(1:10, 3))

xy$Y <- 10*xy$X + rnorm(30, 0, 10)

xy$predicted <- predict(lm(Y ~ X, data = xy))


# X <- sqrt(brain$MRINACount) 
# Y <- sqrt(brain$PIQ)

X <- xy$X
Y <- (xy$Y)


df <- data.frame(X , Y)

# regression
reggig <- glm(Y ~ X, data = df, family = gaussian(link = "identity"))

# empty plot
vX <- seq(min(X) - 0.1, max(X) + 0.1, length = n)
vY <- seq(min(Y) - 10, max(Y) + 10, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n), 
             zlim = c(0, 0.1),
             theta =  - 45, phi = 20, expand = 0.1,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C,  pch = 16, col = "red", cex = 0.6)

# density curves
n <- 10
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))

sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - 10, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + 10, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}

```
 

Симулированный пример, в котором 

$$
y_i = 10x_i + \varepsilon_i \\
\varepsilon \in N(0, 10)
$$

Точки (данные) - это выборки из нескольких "локальных" генеральных совокупностей с нормальным распределением зависимой переменной (каждая совокупность соответствует одному значению предиктора).    

Линия регрессии проходит через средние значения нормальных распределений.   

Параметры прямой подбираются так, чтобы вероятность существования данных при таких параметрах была бы максимальной.   




## Кратко о методе макcимального правдоподобия 

Аналогичная картинка, но с использованием `geom_violin()`
```{r, echo=FALSE, fig.height=5.5}

# xy <- data.frame(X = rep(1:10, 3))
# xy$Y <- 10*xy$X + rnorm(30, 0, 10)
# xy$predicted <- predict(lm(Y ~ X, data = xy))
# 

rand_df <- matrix(rep(NA,100000), ncol = 10)
for(i in 1:10) rand_df[,i] <- rnorm(10000, xy$predicted[i], 10)

rand_df <- data.frame(X = rep(xy$X, each = 10000), Y = as.vector(rand_df))



ggplot(xy, aes(x = X, y = Y)) + geom_violin(data = rand_df, aes(x = factor(X)), adjust = 5 ) + geom_point() + geom_smooth(method = "lm", se = F) + geom_point(data = xy, aes(x = X, y = predicted), color = "red", size = 3) + labs(x = "Предиктор", y = "Зависимая переменная") 
```
 
 

## Метод наименьших квадратов

<div class="columns-2">
<img src="images/OLS.png" width="500" height="400" >

<div class = "footnote">
(из кн. Quinn, Keough, 2002, стр. 85)     
</div>


Остатки (Residuals):            
$$\varepsilon_i = y_i - \hat{y_i}$$

Линия регрессии (подобранная модель) - это та линия, у которой $\sum{\varepsilon_i}^2$ минимальна.



## Подбор модели методом наменьших квадратов с помощью функци `lm()`  
`fit <- lm(formula, data)`

Модель записывается в виде формулы  

Модель | Формула
|-------------|-------------|  
Простая линейная регрессия <br>$\hat{y_i}=b_0 + b_1x_i$ | `Y ~ X` <br> `Y ~ 1 + X` <br> `Y ~ X + 1`  
Простая линейная регрессия <br> (без $b_0$, "no intercept") <br> $\hat{y_i}=b_1x_i$ | `Y ~ -1 + X` <br> `Y ~ X - 1`  
Уменьшенная простая линейная регрессия <br> $\hat{y_i}=b_0$ | `Y ~ 1` <br> `Y ~ 1 - X`  
Множественная линейная регрессия <br> $\hat{y_i}=b_0 + b_1x_i +b_2x_2$ | `Y ~ X1 + X2`  



## Подбор модели методом наменьших квадратов с помощью функци `lm()` {.smaller}
`fit <- lm(formula, data)`

Элементы формул для записи множественных моделей

Элемент формулы | Значение 
|-------------|-------------| 
`:` | Взаимодействие предикторов <br> `Y ~ X1 + X2 + X1:X2`
`*` | Обозначает полную схему взаимодействий <br>  `Y ~ X1 * X2 * X3` <br> аналогично <br> `Y ~ X1 + X2 + X3+ X1:X2 + X1:X3 + X2:X3 + X1:X2:X3` 
`.` | `Y ~ .` <br> В правой части формулы записываются все переменные из датафрейма, кроме `Y` 


## Подберем модель, наилучшим образом описывающую зависимость результатов IQ-теста от размера головного мозга

```{r brain-mod, purl=FALSE}
brain_model <- lm(PIQ ~ MRINACount, data = brain)
brain_model
```


## Как трактовать значения параметров регрессионной модели?

```{r, echo=FALSE, warning=FALSE, fig.align='center',fig.width=9, fig.height=5}
n=100
x <- rnorm(n, 10, 5)
y1 <- 5*x + 50 + rnorm(n, 0, 2)
y2 <- -5*x + 50 + rnorm(n, 0, 2)
y3 <- 0*x + 50 + rnorm(n, 0, 2)
label <- c(rep("Positive slope",n), rep("Negative slope", n), rep("Zero slope", n))
df1 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df1a <- data.frame(intercept = c(50, 50, 50), slope = c(-5,0,5))
pl_1 <- ggplot(data = df1, aes(x = x, y = y, color = label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df1a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Constant intercepts \n Different slopes")

x <- rnorm(n, 10, 5)
y1 <- 5*x + 0 + rnorm(n, 0, 2)
y2 <- 5*x + 30 + rnorm(n, 0, 2)
y3 <- 5*x + 60 + rnorm(n, 0, 2)
label <- c(rep("Intercept = 0",n), rep("Intercept = 30", n), rep("Intercept = 60", n))
df2 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df2a <- data.frame(intercept=c(30, 0, 60), slope=c(5, 5, 5))
pl_2 <- ggplot(df2, aes(x = x, y = y, color=label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df2a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Different intercepts \n Constant slopes")


x <- rnorm(n, 10, 5)
y1 <- 0*x + 0 + rnorm(n, 0, 2)
y2 <- 0*x + 30 + rnorm(n, 0, 2)
y3 <- 0*x + 60 + rnorm(n, 0, 2)
label <- c(rep("Intercept = 0",n), rep("Intercept = 30", n), rep("Intercept = 60", n))
df3 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df3a <- data.frame(intercept = c(30, 0, 60), slope = c(0, 0, 0))
pl_3 <- ggplot(data = df3, aes(x = x, y = y, color=label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df3a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Different intercepts \n Zero slopes")

grid.arrange(pl_1, pl_2, pl_3, nrow = 1)

```


## Как трактовать значения параметров регрессионной модели?

>- Угловой коэффициент (_slope_) показывает *на сколько* _единиц_ изменяется предсказанное значение $\hat{y}$ при изменении на _одну единицу_ значения предиктора ($x$)

>- Свободный член (_intercept_) - величина во многих случаях не имеющая "смысла", просто поправочный коэффициент, без которого нельзя вычислить $\hat{y}$. <br> _NB!_ В некоторых линейных моделях он имеет смысл, например, значения $\hat{y}$ при $x = 0$. 

>- Остатки (_residuals_) - характеризуют влияние неучтенных моделью факторов.

## Вопросы: 
1. Чему равны угловой коэффициент и свободный член полученной модели `brain_model`?       
2. Какое значение IQ-теста предсказывает модель для человека с объемом  мозга равным 900000         
3. Чему равно значение остатка от модели для человека с порядковым номером 10?    

## Ответы
```{r}
coefficients(brain_model) [1]
coefficients(brain_model) [2]

```


## Ответы
```{r}
as.numeric(coefficients(brain_model) [1] + coefficients(brain_model) [2] * 900000)

```

## Ответы
```{r}
brain$PIQ[10] - fitted(brain_model)[10]
residuals(brain_model)[10]

```



## Углубляемся в анализ модели: функция `summary()`
```{r}
summary(brain_model)

```


## Что означают следующие величины?

`Estimate`  
`Std. Error`   
`t value`  
`Pr(>|t|)`   


## Оценки параметров регрессионной модели

Параметр | Оценка      | Стандартная ошибка   
|-------------|--------------------|-------------|   
$\beta_1$ <br> Slope| $b _1 = \frac {\sum _{i=1}^{n} {[(x _i - \bar {x})(y _i - \bar {y})]}}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}$<br> или проще <br> $b_0 = r\frac{sd_y}{sd_x}$ | $SE _{b _1} = \sqrt{\frac{MS _e}{\sum _{i=1}^{n} {(x _i - \bar {x})^2}}}$   
$\beta_0$ <br> Intercept | $b_0 = \bar y - b_1 \bar{x}$  | $SE _{b _0} = \sqrt{MS _e [\frac{1}{n} + \frac{\bar x}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}]}$   
$\epsilon _i$ | $e_i = y_i - \hat {y_i}$ | $\approx \sqrt{MS_e}$   



## Для чего нужны стандартные ошибки?
>- Они нужны, поскольку мы _оцениваем_ параметры по _выборке_
>- Они позволяют построить доверительные интервалы для параметров
>- Их используют в статистических тестах


## Графическое представление результатов {.columns-2 }

```{r, fig.height=5, fig.width=5}
pl_brain + geom_smooth(method="lm") 
```

<br>
<br>
<br>
>Доверительная зона регрессии. В ней с 95% вероятностью лежит регрессионная прямая, описывающая связь в генеральной совокупности. <br>
Возникает из-за неопределенности оценок коэффициентов модели, вследствие выборочного характера оценок.           


## Зависимость в генеральной совокупности {.columns-2 .smaller}

Симулированный пример: Генеральная совокупность, в которой связь между Y и X, описывается следующей зависимостью
$$
y_i = 10 + 10x_i + \varepsilon_i \\
\varepsilon \in N(0, 20)
$$

```{r fig.width=5}
pop_x <- rnorm(1000, 10, 3)
pop_y <- 10 + 10*pop_x + rnorm(1000, 0, 20)
population <- data.frame(x=pop_x, y=pop_y)

ggplot(population, aes(x=x, y=y)) + 
  geom_point(alpha=0.3, color="red") + 
  geom_abline(aes(intercept=10, slope=10), 
              color="blue", size=2)
```

## Зависимости, выявленные в нескольких разных выборках {.columns-2 .smaller}

Линии регрессии, полученные для 100 выборок (по 20 объектов в каждой), взятых из одной и той же генеральной совокупности 

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>



```{r, echo=FALSE, fig.align='center', fig.width=5, purl = FALSE}
samp_coef <- data.frame(b0 = rep(NA, 100), b1=rep(NA, 100))
for(i in 1:100) {
  samp_num <- sample(1:1000, 20)
  samp <- population[samp_num, ]
  fit <- lm(y~x, data=samp)
  samp_coef$b0[i] <- coef(fit)[1]
  samp_coef$b1[i] <- coef(fit)[2]
  
 }

ggplot(population, aes(x=x, y=y)) + geom_point(alpha=0.3, color="red")+ geom_abline(aes(intercept=b0, slope=b1), data=samp_coef) + geom_abline(aes(intercept=10, slope=10), color="blue", size=2)
```



## Доверительные интервалы для коэффициентов уравнения регрессии

```{r}
coef(brain_model)

confint(brain_model)
```

## Для разных $\alpha$ можно построить разные доверительные интервалы

```{r , echo=FALSE, fig.align='center', fig.height=5, fig.width=9}
pl_alpha1 <- pl_brain + geom_smooth(method="lm", level=0.8) + ggtitle(bquote(alpha==0.2))

pl_alpha2 <- pl_brain + geom_smooth(method="lm", level=0.95) + ggtitle(bquote(alpha==0.05))

pl_alpha3 <- pl_brain + geom_smooth(method="lm", level=0.999) + ggtitle(bquote(alpha==0.01))


grid.arrange(pl_alpha1, pl_alpha2, pl_alpha3, ncol=3)

```

## Важно!

Если коэффициенты уравнения регрессии - лишь приблизительные оценки параметров, то предсказать значения зависимой переменной можно только _с нeкоторой вероятностью_.           

## Какое значение IQ можно ожидать у человека с размером головного мозга 900000?

```{r, tidy=TRUE}
newdata <- data.frame(MRINACount = 900000)

predict(brain_model, newdata, 
        interval = "prediction", 
        level = 0.95, se = TRUE)$fit

```

>- При размере мозга 900000 среднее значение IQ будет, с вероятностью 95%, находиться в интервале от 67 до 153.



## Отражаем на графике область значений, в которую попадут 95% предсказанных величин IQ

Подготавливаем данные

```{r , warning=FALSE}
brain_predicted <- predict(brain_model, interval="prediction")
brain_predicted <- data.frame(brain, brain_predicted)
head(brain_predicted)
```


## Отражаем на графике область значений, в которую попадут 95% предсказанных величин IQ

```{r pl-predict, echo=FALSE, fig.align='center', fig.height=5}
pl_brain + 

# 1) Линия регрессии и ее дов. интервал
# Если мы указываем fill внутри aes() и задаем фиксированное значение - появится соотв. легенда с названием.
# alpha - задает прозрачность
  geom_smooth(method = "lm", aes(fill = "Conf.interval"), alpha = 0.4) +
# 2) Интервал предсказаний создаем при помощи геома ribbon ("лента")
# Данные берем из другого датафрейма - из brain_predicted
# ymin и ymax - эстетики геома ribbon, которые задают нижний и верхний край ленты в точках с заданным x (x = MRINACount было задано в ggplot() при создании pl_brain, поэтому сейчас его указывать не обязательно)
  geom_ribbon(data = brain_predicted,  aes(ymin = lwr, ymax = upr, fill = "Conf. area for prediction"), alpha = 0.2) +

# 3) Вручную настраиваем цвета заливки при помощи шкалы fill_manual.
# Ее аргумент name - название соотв. легенды, values - вектор цветов
  scale_fill_manual(name = "Intervals", values = c("green", "gray")) +

# 4) Название графика
  ggtitle("Confidence interval \n and confidence area for prediction")
```


## Код для построения графика {.smaller}


```{r echo=TRUE, eval=FALSE}
pl_brain + 

# 1) Линия регрессии и ее дов. интервал
# Если мы указываем fill внутри aes() и задаем фиксированное значение - 
#  появится соотв. легенда с названием.
# alpha - задает прозрачность
  geom_smooth(method = "lm", aes(fill = "Conf.interval"), alpha = 0.4) +
# 2) Интервал предсказаний создаем при помощи геома ribbon ("лента")
# Данные берем из другого датафрейма - из brain_predicted
# ymin и ymax - эстетики геома ribbon, которые задают нижний и верхний 
# край ленты в точках с заданным x (x = MRINACount было задано в ggplot() 
# при создании pl_brain, поэтому сейчас его указывать не обязательно)
#

geom_ribbon(data = brain_predicted,  aes(ymin = lwr, ymax = upr, fill = "Conf. area for prediction"), alpha = 0.2) +
# 3) Вручную настраиваем цвета заливки при помощи шкалы fill_manual.
# Ее аргумент name - название соотв. легенды, values - вектор цветов
  scale_fill_manual(name = "Intervals", values = c("green", "gray")) +
# 4) Название графика
  ggtitle("Confidence interval \n and confidence area for prediction")
```


## Важно! {.columns-2 .smaller}

Модель "работает" только в том диапазоне значений независимой переменной ($x$), для которой она построена (интерполяция). Экстраполяцию надо применять с большой осторожностью.

```{r, fig.align='center', fig.height=4, fig.width=9, echo=FALSE}
pl_brain + 
  geom_ribbon(data=brain_predicted, aes(y=fit, ymin=lwr, ymax=upr, fill = "Conf. area for prediction"), alpha=0.2) + 
  geom_smooth(method="lm", aes(fill="Conf.interval"), alpha=0.4) + 
  scale_fill_manual("Intervals", values = c("green", "gray")) + 
  ggtitle("Confidence interval \n and confidence area for prediction")+ xlim(600000, 1300000) + geom_text(label="Interpolation", aes(x=950000, y=100)) + geom_text(label="Extrapolation", aes(x=700000, y=70)) +
geom_text(label="Extrapolation", aes(x=1200000, y=70))

``` 

</div>


## Итак, что означают следующие величины?

>- `Estimate` 
>- Оценки праметров регрессионной модели 
>- `Std. Error`   
>- Стандартная ошибка для оценок    
>- Осталось решить, что такое `t value`, `Pr(>|t|)`




## Summary

> - Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 x _i + \epsilon _i$
- Параметры модели оцениваются на основе выборки
- В оценке коэффициентов регрессии и предсказанных значений существует неопределенность: необходимо вычислять доверительный интервал. 
- Доверительные интервалы можно расчитать, зная стандартные ошибки.  

## Что почитать

+ Гланц, С., 1998. Медико-биологическая статистика. М., Практика
+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014
+ Diez, D.M., Barr, C.D. and Çetinkaya-Rundel, M., 2015. OpenIntro Statistics. OpenIntro.
+ Zuur, A., Ieno, E.N. and Smith, G.M., 2007. Analyzing ecological data. Springer Science & Business Media.
+ Quinn G.P., Keough M.J. 2002. Experimental design and data analysis for biologists
+ Logan M. 2010. Biostatistical Design and Analysis Using R. A Practical Guide


