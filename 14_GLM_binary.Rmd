---
title: "Обобщенные линейные модели с бинарным откликом"
subtitle: "Линейные модели..."
author: "Вадим Хайтов, Марина Варфоломеева"
institute: "Кафедра Зоологии беспозвоночных, Биологический факультет, СПбГУ"
fontsize: 10pt
classoption: 't,xcolor=table'
language: russian, english
output:
  beamer_presentation:
    theme: default
    toc: no
    colortheme: beaver
    latex_engine: xelatex
    slide_level: 2
    fig_crop: false
    highlight: tango
    includes:
      in_header: ./includes/header.tex
---

```{r setup, include=FALSE, cache=FALSE, purl=FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library (knitr)
library (scales)
# chunk default options
opts_chunk$set(fig.show='hold', size='footnotesize', comment='#', 
               fig.width=4.5, out.width='3in',
               fig.height=2.25, out.height='1.5in',
               cache=FALSE,
               dev='cairo_pdf',
               warning=FALSE, message=FALSE)
# Тема для иллюстраций
library (ggplot2)
library (gridExtra)
theme_spbu <- function () { 
    theme_bw(base_size = 11, base_family='GT Eesti Pro Display') + 
    theme(plot.title = element_text(face = 'bold', colour = '#1e1e1e'), 
          panel.border = element_rect(fill = NA, size = 0.75), 
          panel.grid = element_blank(),
          strip.background = element_blank(), 
          strip.text = element_text(face = 'bold', colour = '#1e1e1e'))
}
# op <- par()
# par_spbu <- par(
#     mar = c(3, 3, 2, 0.5), # Dist' from plot to side of page
#     mgp = c(2, 0.4, 0), # Dist' plot to label
#     las = 1, # Rotate y-axis text
#     tck = -.01, # Reduce tick length
#     xaxs = 'i', yaxs = 'i', 
#     family = 'GT Eesti Pro Display',
#     mex = 0.8,
#     mgp = c(2, .5, 0),
#     tcl = -0.3) # Remove plot padding
#source(file = 'includes/spbu_cols.R')
#source(file = 'includes/functions.R')
```

```{r echo=FALSE, eval=TRUE, purl=TRUE}
# чтобы избежать scientific notation
options(scipen = 6, digits = 3)
```

# Бинарные переменные вокруг нас


## Числа и События

До сих пор в качестве переменной отклика мы рассматривали числовые данные. 

- Содержание сухого вещества в икре 
- Вес младенцев 
- Объем легких
- Число посещений цветков опылителями. 

А как быть, если мы хотим проанализировать связь появления того или иного **события** (произошло или не произошло) с некоторыми предикторами? 




## События и предикторы

В зависимости от предикторов события могут происходить чаще или реже -- логика, совпадающая с логикой связи количественной переменной отклика с набором предикторов.

Например, по мере роста температуры воздуха летом чаще будут встречаться люди в шортах: событие "встретился человек в шортах" положительно связано с температурой воздуха.

Событие "покупка автомобиля" явно связана с предиктором "количество денег на счете", однако эта связь может быть совсем непростой.


## Бинарные данные вокруг нас

Бинарные данные -- очень распространенный тип зависимых переменных


- Кто-то в результате лечебных процедур выжил или умер       
- Обследованное животное заражено паразитами или здорово     
- Футбольная команда выиграла или проиграла     
- Блюдо вкусное или невкусное

Все эти события могут быть связаны с самыми разными предикторами и эту связь можно описать с помощью регрессионных моделей.

Обобщенные линейные модели позволяют моделировать в том числе и бинарные данные.



# Пример -- морские звезды и мидии {.t}



## Различают ли морские звезды два вида мидий?

Атлантические мидии (*Mytilus edulis*) коренной для Белого моря вид, но недавно туда вселились мидии другого вида -- тихоокеанские мидии (*M.trossulus*). 

\columnsbegin
\column{0.48\textwidth}

\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{images/Sea_star.png}

\column{0.48 \textwidth}

Вселенец имеет меньшую промысловую значимость и потенциально может влиять на структуру экосистемы. Важно понять, что регулирует их численность. Наиболее значимый фактор -- это морские звезды, питающиеся мидиями. 

- Различают ли морские звезды два вида мидий? 
- Различают ли хищники мидий разных размеров?

\tiny{Данные: Khaitov et al, 2018}

\columnsend


## Тонкости дизайна эксперимента

Морских звезд вместе с мидиями двух видов сажали в контейнеры. Через четыре дня совместного существования с хищником регистрировали состояние мидий.

\columnsbegin
\column{0.48\textwidth}

\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{images/Containers.jpg}


\column{0.48 \textwidth}

Зависимая переменная:    
- `Outcome` -- состояние мидий  
("eaten" -- съедена, "not_eaten" -- живая ) 

Предикторы в фокусе исследования:   
- `Sp` -- вид мидий ("Ed" -- коренной вид, "Tr" -- вселенец),  
- `L` -- размер мидий (мм).

Чего не хватает?

\columnsend


## Как быть с контейнерами?

 В этом эксперименте, помимо интересующих нас дискретного фактора `Sp` (вид мидии) и непрерывного предиктора `L` (размер), есть еще один фактор `Box`.     

Этот фактор нас не интересует, но его нельзя не учитывать.     

Мы должны включить в модель переменную `Box` в качестве дискретного фактора с 4 уровнями.    

В лекциях, посвященных **смешанным линейным моделям**, мы научим вас, как включать в модель подобные факторы более правильным способом.  



## Читаем данные

```{r}
astr <- read.csv('data/aster_mussel.csv', header = TRUE)
head(astr)
```

Номер экспериментального контейнера закодирован числами,  
поэтому превращаем его в фактор. 

```{r}
astr$Box <- factor(astr$Box)
```

## Знакомимся с данными

Нет ли пропущенных значений?

```{r}
colSums(is.na(astr))
```

Каковы объемы выборок?

```{r}
table(astr$Box)
```

## Нет ли коллинеарности 

\columnsbegin
\column{0.6\textwidth}

```{r}
library(ggplot2); theme_set(theme_bw()); library(cowplot)

Pl_Sp <- ggplot(astr, aes(x = Sp, y = L)) + geom_boxplot()
Pl_Box <- ggplot(astr, aes(x = Box, y = L)) + geom_boxplot()
plot_grid(Pl_Sp, Pl_Box, ncol = 2)
```
\column{0.35\textwidth}

\vspace{7\baselineskip}

Размер распределен более-менее равномерно.
Коллинеарности нет.

\columnsend

## Есть ли выбросы?

\columnsbegin
\column{0.6\textwidth}

```{r, fig.height=1.88, out.height='1.25in'}
ggplot(astr, aes(y = 1:nrow(astr))) + geom_point(aes(x = L) )
```

\column{0.35\textwidth}

\vspace{3\baselineskip}

Выбросов нет.

\columnsend


## Кодирование бинарной переменной 

До сих пор зависимая переменная была числом,  
а в данном случае `Outcome` -- это текстовая переменная.

Бинарную переменную надо перекодировать в виде нулей и единиц:

- 1 -- мидию съели,   
- 0 -- мидию не съели.

```{r}
astr$Out <- ifelse(test = astr$Outcome == 'eaten', yes = 1,  no = 0)
```


# Простой линейной регрессией не обойтись {.t}


## Что мы хотим построить?

Наша задача -- построить модель, описывающую связь между переменной-откликом (съедена мидия или нет) и тремя предикторами: `Sp`, `L` и `Box`  

Если бы мы строили GLM с нормальным распределением отклика, то она имела бы следующий вид:

$Out_i \sim N(\mu_i, \sigma)$

$E(Out_i) = \mu_i$

$\mu_i = \eta_i$ -- функция связи "идентичность"

$\eta_i = b_0 + b_1Sp_{Tr\,i} + b_2L_i + b_3Box_{2\,i}+b_3Box_{3\,i} + Interactions$,

где $Interactions$ --- это все взаимодействия.

## Лобовая атака?

Строим модель по накатанной дороге.

Мы не знаем, взаимодействуют ли дискретные факторы `Box`, `Sp` и непрерывный предиктор `L`.

В полной модели мы должны учесть как влияние самих предикторов, так и влияние их взаимодействия.

```{r}
mod_norm <- glm(Out ~ Sp * L * Box, data = astr)
```

Все посчиталось... 

## Посмотрим что получилось

\columnsbegin
\column{0.6\textwidth}

\small

```{r fig.height=1.25*1.5, out.height='1.25in'}
library(dplyr)
new_data <- astr %>% group_by(Sp, Box)%>%
  do(data.frame(L = seq(min(.$L), max(.$L), length.out = 100)))
new_data$fit <- predict(mod_norm, newdata = new_data) # Предсказанные значения
ggplot(new_data, aes(x = L, y = fit)) + 
  geom_line(aes(group = Box)) + facet_wrap(~ Sp, ncol = 2) + 
  geom_point(data = astr, aes(x = L, y = Out), size = 0.5, color = 'blue')
```

\column{0.35\textwidth}

\vspace{7\baselineskip}

Во-первых, непонятно, что за величина отложена по оси OY.

\columnsend

## Диагностика модели

\columnsbegin
\column{0.6\textwidth}

```{r}
mod_norm_diag <- fortify(mod_norm)
ggplot(mod_norm_diag, aes(x = .fitted, y = .stdresid)) +  
  geom_point() + geom_vline(xintercept = 0)
```

\column{0.35\textwidth}

\vspace{5.5\baselineskip}

Во-вторых, модель предсказывает отрицательные значения.

Простая линейная модель **категорически не годится**!

\columnsend

## Логистическая кривая  {.t}


## Бинарные данные можно представлять и иначе

Бинарные данные очень неудобны для работы.
Вместо того, чтобы моделировать наличие нулей и единиц, мы будем моделировать вероятности получения единиц.

Появляется новое обозначение:

- $\pi_i$ -- вероятность события $y_i = 1$ при данных условиях,      
- $1 - \pi_i$ -- вероятность альтернативного события $y_i = 0$.

\vspace{\baselineskip}

$\pi_i$ -- непрерывная величина, варьирующая от 0 до 1.


## Симулированный пример: \newline От дискретных значений к оценкам вероятностей

От 1 и 0 (слева) можно перейти к $\pi_i$ -- оценкам вероятности положительных исходов (справа).

Мы можем проиллюстрировать этот переход, изобразив доли в общем количестве исходов **при данном значении предиктора** $p_{y = 1 | x_i}$ (красные точки). И $\pi$, и $p_{y = 1 | x_i}$ варьируют от 0 до 1.

```{r, echo=FALSE, fig.width=4*1.5, out.width='4in'}
set.seed(123456)
x <- 0:20 

dat <- data.frame(X = rep(x, each = 10))

d <- exp(-5 + 0.5 * dat$X)
pi <- d/(1 + d) 

dat$Out <- rbinom(nrow(dat), 1, pi)

P1_discr <- ggplot(dat, aes(x = X, y = Out)) + geom_point(position = position_jitter(width = 0.11, height = 0.011), alpha = 0.2) + labs(x = 'Предиктор', y = 'Зависимая переменная \n(1 или 0)') + theme_spbu() + geom_hline(yintercept = 0, linetype = 2) + geom_hline(yintercept = 1, linetype = 2) + scale_y_continuous(breaks = seq(-0.25, 1.25, by = 0.25))

library(doBy)
library(cowplot)
Probs <- summaryBy(Out ~ X, data = dat, FUN = mean) 

Pl_prob <- ggplot(data = Probs, aes(x = X, y = Out.mean)) + geom_point(size = 2, color ='orangered') + labs(x = 'Предиктор', y = 'Вероятность получения 1')  + theme_spbu() + geom_hline(yintercept = 0, linetype = 2) + geom_hline(yintercept = 1, linetype = 2) + scale_y_continuous(breaks = seq(-0.25, 1.25, by = 0.25))

plot_grid(P1_discr, Pl_prob, ncol = 2)
```


## Симулированный пример: \newline Можно ли подобрать простую линейную регрессию?

\columnsbegin
\column{0.48\textwidth}


```{r, echo=FALSE, fig.width=2.7*1.5, out.width='2in'}
mod_norm <- lm(Out.mean ~ X, data = Probs)
coef_mod_norm <- round(as.numeric(coef(mod_norm)),2)
Pl_prob + geom_abline(slope = coef_mod_norm[2], intercept = coef_mod_norm[1]) + coord_cartesian(ylim = c(-0.25, 1.25), xlim = c(-2, 22)) 
```


\column{0.48\textwidth}

Связь зависимой переменной с предиктором можно было бы описать прямой:

$\pi_i = \beta_0 + \beta_1x_i$

Но! Вероятность события, может принимать значения только от 0 до 1.  
А прямая линия ничем не ограничена и может выходить за пределы интервала [0, 1]. 


\columnsend

## Симулированный пример: Логистическая кривая 

\columnsbegin
\column{0.48\textwidth}

```{r, echo=FALSE, fig.width=2.7*1.5, out.width='2in'}
Pl_logistic <- Pl_prob + geom_smooth(method = 'glm', method.args = list(family='binomial'), se = FALSE)  
Pl_logistic
```

\column{0.48\textwidth}

Связь вероятности положительного исхода и значений предиктора можно описать логистической кривой:

$$ 
\pi_i = \frac{e^{\beta_0 + \beta_1x_i}}{1 + e^{\beta_0 + \beta_1x_i}}
$$
Логистическая кривая удобна для описания вероятностей, т.к. ее значения лежат в пределах от 0 до 1.

\columnsend

## Симулированный пример: Логистическая кривая 

\columnsbegin
\column{0.48\textwidth}

```{r, echo=FALSE, fig.width=2.7*1.5, out.width='2in'}
P1_discr + geom_smooth(method = 'glm', method.args = list(family='binomial'), se = FALSE)  
```

\column{0.48\textwidth}

В реальной жизни нам не потребуется даже рассчитывать доли положительных исходов от общего количества.

Благодаря GLM мы сможем оценить вероятности непосредственно по исходным данным.

\columnsend

# Шансы и логиты  {.t}



## Шансы -- еще один способ выразить бинарную переменную отклика


В обыденной речи мы часто используем фразы подобные такой:

"Шансы на победу 1 к 3": в одном случае выигрыш в трех проигрыш

Шансы -- это тоже оценка вероятности события. Шансы показывают сколько в данной системе положительных исходов и сколько отрицательных.


## Отношение шансов

Шансы (odds) часто представляют в виде отношения шансов (odds ratio):  $odds=\frac{n_{+}}{n_{-}}$

Если отношение шансов > 1, то вероятность наступления события выше, чем вероятность того, что оно не произойдет.  Если отношение шансов < 1, то наоборот.



Если можно оценить вероятность положительного события, то отношение шансов выглядит так : $odds=\frac{\pi}{1-\pi}$  

Отношение шансов варьирует от 0 до $+\infty$.


## Отношение шансов

Если отношение шансов = 1, то вероятность того, что событие произойдет равно вероятности того, что событие не произойдет. 

Асимметрия: отношение шансов от 1 до $+\infty$ говорит о том, что вероятность того, что событие произойдет, выше, чем вероятность того, что оно не произойдет, но если наоборот, то отношение шансов "зажато" между 0 и 1. 


## Логиты

Отношение шансов можно преобразовать в _Логиты_ (logit):

$$
ln(odds)=ln(\frac{\pi}{1-\pi})
$$ 

Значения логитов -- это трансформированные оценки вероятности события.

Логиты варьируют от  $-\infty$ до $+\infty$. 

Логиты симметричны относительно 0, т.е. $ln(1)$. 

Для построения моделей в качестве зависимой переменной удобнее брать логиты.


# Немного алгебры: Логиты в качестве зависимой переменной  {.t}



## Докажем, что логит преобразование линеаризует логистическую кривую

Когда предиктор один, логистическая модель принимает такую форму:

$\pi = \cfrac{e ^ {\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

Обозначим для краткости $\beta_0 + \beta_1 x \equiv z$

Давайте докажем, что логит преобразование
$logit(\pi) = ln\Big(\cfrac{\pi}{1 - \pi}\Big)$
сделает логистическую функцию линейной, т.е. что

$ln\Big(\cfrac{\pi}{1 - \pi}\Big) = z$

## Подставим выражение для $\pi$ в формулу логита

$ln(\frac{\pi}{1-\pi}) = ln(\frac{\frac{e^z}{1+e^z}}{1-\frac{e^z}{1+e^z}})$

Логарифм отношения равен разности логарифмов, тогда:

$ln(\frac{e^z}{1+e^z}) - ln({1-\frac{e^z}{1+e^z}})$

Вторую дробь можно упростить:

\pause

$ln(\frac{e^z}{1+e^z}) - ln({\frac{1+e^z - e^z}{1+e^z}}) = ln(\frac{e^z}{1+e^z}) - ln({\frac{1}{1+e^z}})$

## Продолжаем преобразования

$ln(\frac{e^z}{1+e^z}) - ln({\frac{1}{1+e^z}}) = ln(e^z) - ln(1+e^z) - (ln(1) -ln(1+e^z))$

\pause
$ln(e^z) - ln(1+e^z) - 0 +ln(1+e^z) = ln(e^z) = z$

$ln(\frac{\pi(x)}{1-\pi(x)})= z = \beta_0 + \beta_1 x$

Т.е. после логит-преобразования логистическая кривая становится прямой.


## Связывающая функция (link function)

**Мы уже знаем**: Для линеаризации связи между предикторами и зависимой переменной применяется связывающая функция.

Функция логит-преобразования  $g(E(y))=ln(\frac{\pi}{1-\pi})$ это одна из возможных связывающих функций, применяемых для анализа бинарных переменных отклика.

Другие связывающие функции: probit, cloglog.


## Логика математических преобразований

1. От дискретной оценки событий (1 или 0) переходим к оценке вероятностей.
2. Связь вероятностей с предиктором описывается логистической кривой.
3. Если при помощи функции связи перейти от вероятностей к логитам, то связь с предиктором будет описываться прямой линией.
4. Параметры линейной модели для такой прямой можно оценить при помощи линейной модели.

\vspace{\baselineskip}

Теперь мы готовы сформулировать модель в математическом виде.

## GLM с биномиальным распределением отклика

$y_i \sim Binomial(n = 1, \pi_i)$

$E(y_i) = \pi_i = \cfrac{e ^ {\beta_0 + \beta_1 x_{1} + ... + \beta_{p-1}~x_{p- 1}}}{1 + e^{\beta_0 + \beta_1 x_{1} + ... + \beta_{p-1}~x_{p- 1}}}$

$ln(\cfrac{\pi_i}{1 - \pi_i}) = \eta_i$ --- функция связи логит, переводит вероятности в логиты.

$\eta_i = \beta_0 + \beta_1 x_{1\,i} + ... + \beta_{p-1\,i}~x_{p- 1\,i}$

\vspace{1\baselineskip}

Чтобы перейти обратно от логитов к вероятностям,
применяется логистическое преобразование
(это функция, обратная функции связи):

$\pi_i = \cfrac{e ^ {\eta_i}}{1 + e^{\eta_i}}$


# Вернемся к морским звездам и мидиям  


## GLM с биномиальным распределением отклика

$Out_i \sim Binomial(n = 1, \pi_i)$

$E(Out_i) = \pi_i$


$ln(\frac{\pi_i}{1 - \pi_i}) = \eta_i$


$\pmb{\eta} = \mathbf{X}\pmb{\beta}$


Полная модель в изучаемой системе включает много членов:

- Главные предикторы: $Sp$, $L$, $Box$
- Взаимодействия первого порядка: $Sp$:$L$, $Sp$:$Box$, $L$:$Box$
- Взаимодействия второго порядка: $Sp$:$L$:$Box$


<!-- $\eta_i = b_0 + b_1Sp_{Tr_i} + b_2L_i + b_3Box_{2_i} + b_3Box_{3_i} + b_4Box_{4_i} + b_5LBox_{2_i} + b_6LBox_{3_i} + b_7LBox_{4_i} + b_8Sp_{Tr_i}Box_{2_i} + b_9Sp_{Tr_i}Box_{3_i} + b_10Sp_{Tr_i}Box_{4_i}$ -->

```{r}
mod <- glm(Out ~ Sp*L*Box, family = binomial(link = 'logit'), data = astr)
```


## Анализ девиансы для полной модели

```{r}
library(car)
Anova(mod)
```

Эту модель можно упростить!


## Упрощение модели: Шаг 1

```{r}
drop1(mod, test = 'Chi')
```


```{r}
mod2 <- update(mod, . ~ . - Sp:L:Box)
```



## Упрощение модели: Шаг 2

```{r}
drop1(mod2, test = 'Chi')

```

```{r}
mod3 <- update(mod2, . ~ . - Sp:L)
```



## Упрощение модели: Шаг 3

```{r}
drop1(mod3, test = 'Chi')

```

```{r}
mod4 <- update(mod3, . ~ . - L:Box)
```



## Упрощение модели: Шаг 4

```{r}
drop1(mod4, test = 'Chi')

```

```{r}
mod5 <- update(mod4, . ~ . - Sp:Box)
```


## Упрощение модели: Шаг 5

```{r}
drop1(mod5, test = 'Chi')

```

```{r}
mod6 <- update(mod5, . ~ . - Box)
```

## Упрощение модели: Шаг 6


```{r}
drop1(mod6, test = 'Chi')

```

Больше никаких предикторов исключать нельзя: `mod6` -- финальная модель.

## AIC для финальной модели

```{r}
AIC(mod, mod2, mod3, mod4, mod5, mod6)
```

Информационный критерий Акайке показывает, что по мере удаления предикторов модель становится лучше.

Финальная модель (`mod6`) лучше, чем полная модель, с которой мы начинали.


## Смысл коэффициентов в моделях с бинарной переменной отклика  



## Что за модель мы построили?

\columnsbegin
\column{0.6\textwidth}

\footnotesize
```{r}
summary(mod6)
```

\column{0.35\textwidth}

$Out_i \sim Binomial(n = 1, \pi)$

$E(Out_i) = \pi_i$

$ln(\frac{\pi_i}{1 - \pi_i}) = \eta_i$

$\eta_i = 0.399 + 1.07 Sp_{\,Tr\,i} -0.113 L_i$

\columnsend

## Что означают коэффициенты модели?

$Out_i \sim Binomial(n = 1, \pi)$

$E(Out_i) = \pi_i$

$ln(\frac{\pi_i}{1 - \pi_i}) = \eta_i$

$\eta_i = 0.399 + 1.07 Sp_{\,Tr\,i} -0.113 L_i$

\vspace{\baselineskip}

- $b_0$ -- интерсепт, логарифм отношения шансов для базового уровня дискретного фактора. 
- $b_1$ -- __на сколько единиц изменяется логарифм отношения шансов__ (logit) для данного уровня (`Tr`) дискретного фактора `Sp`  по сравнению с базовым уровнем (`Ed`).
- $b_2$ -- __на сколько единиц изменяется логарифм отношения шансов__ (logit), если значение предиктора (`L`) изменяется на единицу.

## Немного алгебры для понимания сути коэффициентов 

Предположим, что у нас в модели есть только один непрерывный предиктор $x$.

Посмотрим, как изменится предсказанные моделью значения, если значение непрерывного предиктора изменится на 1.

Мы знаем, что в терминах логитов модель выглядит вот так:

$\eta = ln(\frac{\pi}{1 - \pi}) = ln(odds)$ 

Тогда разница между значениями $\eta$ для $x+1$ и $x$ -- это логарифм соотношения шансов при этих значениях предиктора:

$\eta_{x+1} - \eta_{x} = ln(odds_{x+1}) - ln(odds_x)  = ln(\frac{odds_{x+1}}{odds_x})$

## Продолжим преобразования

$ln(\frac{odds_{x+1}}{odds_x}) = b_0 + b_1(x+1) - b_0 - b_1x = b_1$

$ln(\frac{odds_{x+1}}{odds_x}) = b_1$

$\frac{odds_{x+1}}{odds_x} = e^{b_1}$

Полученная величина $e^{b_1}$ показывает, __во сколько раз изменится отношение шансов__ при увеличении предиктора на единицу.

Для дискретных факторов $e^{b_1}$ покажет, во сколько раз различается отношение шансов для данного уровня по сравнению с базовым.  

## Геометрическая интерпретация коэффициентов 

```{r echo=FALSE, fig.width=5*1.5, out.width='4.8in', fig.height=2.5*1.55, out.height='2.5in'}
# надо нанести горизонтальную линию 0.5. Синим сделать b0<0 а красным b0>0

X <- data.frame(x = seq(-10, 10))
logit <- function(x, b0, b1){
  exp(b0 + b1 * x) / (1 + exp(b0 + b1 * x))
}
lin <- function(x, b0, b1) {
  b0 + b1*x
}

gg_shift_intercept <- ggplot(X, aes(x = x)) + 
  stat_function(fun = lin, args = list(b0 = 3, b1 = 0.5), aes(colour = '#d73027'), size = 1.1) +
  stat_function(fun = lin, args = list(b0 = 0, b1 = 0.5), aes(colour = 'grey60'), size = 1) +
  stat_function(fun = lin, args = list(b0 = -3, b1 = 0.5), aes(colour = '#4575b4'), size = 0.9) + 
  scale_color_identity(name = 'Интерсепт', breaks = c('#d73027', 'grey60', '#4575b4'), labels = c('b0 = 3', 'b0 = 0', 'b0 = -3'), guide = 'legend') + 
  geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = 0, linetype = 1, size = 0.3) +
  coord_cartesian(xlim = c(-10, 10)) +  theme_spbu() + theme(legend.position = 'none') +
  labs(y = 'Логиты')
# gg_shift_intercept

gg_negative_slope <- ggplot(X, aes(x = x)) + 
    stat_function(fun = lin, args = list(b0 = 0, b1 = -2), aes(colour = '#4575b4'), size = 1) +
  stat_function(fun = lin, args = list(b0 = 0, b1 = -1), aes(colour = '#74add1'), size = 1) +
  stat_function(fun = lin, args = list(b0 = 0, b1 = -0.5), aes(colour = '#abd9e9'), size = 1) +
    scale_color_identity(name = 'Отрицательный угловой коэффициент', breaks = c('#abd9e9', '#74add1', '#4575b4'), labels = c('b1 = -0.5', 'b1 = -1', 'b1 = -2'), guide = 'legend') + 
    geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = 0, linetype = 1, size = 0.3) +
  coord_cartesian(xlim = c(-6, 6)) +  theme_spbu() + theme(legend.position = 'none') +
  labs(y = 'Логиты')
# gg_negative_slope

gg_positive_slope <- ggplot(X, aes(x = x)) + 
  stat_function(fun = lin, args = list(b0 = 0, b1 = 0.5), aes(colour = '#fdae61'), size = 1) +
    stat_function(fun = lin, args = list(b0 = 0, b1 = 1), aes(colour = '#f46d43'), size = 1) +
  stat_function(fun = lin, args = list(b0 = 0, b1 = 2), aes(colour = '#d73027'), size = 1) +
      scale_color_identity(name = 'Положительный угловой коэффициент', breaks = c('#fdae61', '#f46d43', '#d73027'), labels = c('b1 = 0.5', 'b1 = 1', 'b1 = 2'), guide = 'legend') + 
    geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = 0, linetype = 1, size = 0.3) +
  coord_cartesian(xlim = c(-6, 6)) +  theme_spbu() + theme(legend.position = 'none') +
  labs(y = 'Логиты')
# gg_positive_slope

gg_shift_intercept_pi <- ggplot(X, aes(x = x)) + 
  stat_function(fun = logit, args = list(b0 = 3, b1 = 0.5), aes(colour = '#d73027'), size = 1.1) +
  stat_function(fun = logit, args = list(b0 = 0, b1 = 0.5), aes(colour = 'grey60'), size = 1) +
  stat_function(fun = logit, args = list(b0 = -3, b1 = 0.5), aes(colour = '#4575b4'), size = 0.9) + 
  scale_color_identity(name = 'Интерсепт', breaks = c('#d73027', 'grey60', '#4575b4'), labels = c('b0 = 3', 'b0 = 0', 'b0 = -3'), guide = 'legend') + 
  geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = c(0, 0.5, 1), linetype = c(2, 1, 2), size = rep(0.3, 3)) +
  coord_cartesian(xlim = c(-10, 10)) +  theme_spbu() + theme(legend.position = 'bottom') +
  guides(colour = guide_legend(ncol = 1)) +
  labs(y = 'Вероятности')
# gg_shift_intercept_pi

gg_negative_slope_pi <- ggplot(X, aes(x = x)) + 
    stat_function(fun = logit, args = list(b0 = 0, b1 = -2), aes(colour = '#4575b4'), size = 1) +
  stat_function(fun = logit, args = list(b0 = 0, b1 = -1), aes(colour = '#74add1'), size = 1) +
  stat_function(fun = logit, args = list(b0 = 0, b1 = -0.5), aes(colour = '#abd9e9'), size = 1) +
    scale_color_identity(name = 'Отрицательный \nугловой \nкоэффициент', breaks = c('#abd9e9', '#74add1', '#4575b4'), labels = c('b1 = -0.5', 'b1 = -1', 'b1 = -2'), guide = 'legend') + 
    geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = c(0, 0.5, 1), linetype = c(2, 1, 2), size = rep(0.3, 3)) +
  coord_cartesian(xlim = c(-6, 6)) +  theme_spbu() + theme(legend.position = 'bottom') +
  guides(colour = guide_legend(ncol = 1)) +
  labs(y = 'Вероятности')
# gg_negative_slope_pi

gg_positive_slope_pi <- ggplot(X, aes(x = x)) + 
  stat_function(fun = logit, args = list(b0 = 0, b1 = 0.5), aes(colour = '#fdae61'), size = 1) +
    stat_function(fun = logit, args = list(b0 = 0, b1 = 1), aes(colour = '#f46d43'), size = 1) +
  stat_function(fun = logit, args = list(b0 = 0, b1 = 2), aes(colour = '#d73027'), size = 1) +
      scale_color_identity(name = 'Положительный \nугловой \nкоэффициент', breaks = c('#fdae61', '#f46d43', '#d73027'), labels = c('b1 = 0.5', 'b1 = 1', 'b1 = 2'), guide = 'legend') + 
    geom_vline(xintercept = 0, size = 0.3) +
  geom_hline(yintercept = c(0, 0.5, 1), linetype = c(2, 1, 2), size = rep(0.3, 3)) +
  coord_cartesian(xlim = c(-6, 6)) +  theme_spbu() + theme(legend.position = 'bottom') +
  guides(colour = guide_legend(ncol = 1)) +
  labs(y = 'Вероятности')
# gg_positive_slope_pi

plot_grid(
  gg_negative_slope, gg_positive_slope, gg_shift_intercept, 
  gg_negative_slope_pi, gg_positive_slope_pi, gg_shift_intercept_pi,
  nrow = 2, rel_heights = c(0.37, 0.63), align = 'v')
```

## Смысл интерсепта $b_0$

Величина $e^{b_0}$ показывает отношение шансов для события, когда все предикторы равны нулю.

Когда предикторы физически не могут принимать нулевые значения,  у этой величины нет смысла.

Но если произведена стандартизация предикторов, то смысл появится. У стандартизованных величин среднее значение равно нулю. Поэтому $e^{b_0}$ покажет соотношение шансов для события при средних значениях предикторов.




## Трактуем коэффициенты модели

$\eta_i = 0.399 + 1.07I_{Tr,i} -0.113L_i$

\vspace{\baselineskip}

- При увеличении длины тела мидии на 1 мм отношения шансов быть съеденной увеличатся в $e^{-0.113}$ = `r exp(coef(mod6)[3])` раза. То есть мидия, имеющая больший размер, имеет меньше шансов быть съеденной (больше шансов на выживание)

- Отношение шансов быть съеденной у мидии, относящиеся к группе `Tr` дискретного фактора `Sp`, в $e^{1.07}$ =  `r exp(coef(mod6)[2])` раза выше, чем у мидии относящейся к базовому уровню (`Ed`). То есть вероятность выжить у мидии из группы `Tr` меньше, чем у мидии из группы `Ed`.


# Диагностика модели с бинарным откликом 



## Условия применимости GLM с бинарной переменной-откликом

- Случайность и независимость наблюдений.
- Линейность связи переменной отклика с предиктором (с учетом связывающей функции).
- Отсутствие сверхдисперсии (форма связи среднего с дисперсией должна быть как у величины с биномиальным распределением).
- Отсутствие коллинеарности предикторов.

## Линейность связи

\small

Мы должны выяснить, нет ли криволинейного паттерна в остатках. Самый простой способ -- это построить график остатков от предсказанных значений и наложить на него сглаживающую функцию, подобранную методом loess.  

\vspace{-\baselineskip}

\columnsbegin
\column{0.6\textwidth}

```{r fig.height=1.3*1.5, out.height='1.3in'}
mod6_diag <- data.frame(.fitted = predict(mod6, type = 'response'),
                         .resid_p = resid(mod6, type = 'pearson'))
ggplot(mod6_diag, aes(y = .resid_p, x = .fitted)) + geom_point() +
  geom_hline(yintercept = 0) +  geom_smooth(method = 'loess')
```

\column{0.35\textwidth}

\vspace{5\baselineskip}

Явного криволинейного паттерна нет.

\columnsend

## Проверка на сверхдисперсию

Важное свойство биномиального распределения -- это зависимость между матожиданием и дисперсией.

Мат.ожидание -- $E(y_i) = \pi_i$  
Дисперсия -- $var(y_i) = \pi_i (1-\pi_i)$  

То есть в распределении остатков не должно наблюдаться сверхдисперсии (overdispersion). 

## Еще раз смотрим на результаты

\footnotesize
```{r, eval = FALSE}
summary(mod6)
```

```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept)    0.399      0.875    0.46   0.6483   
SpTr           1.070      0.379    2.82   0.0047 **
L             -0.113      0.035   -3.24   0.0012 **
--
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 212.58  on 285  degrees of freedom
Residual deviance: 191.24  on 283  degrees of freedom
AIC: 197.2

Number of Fisher Scoring iterations: 5

```

Важная строчка

```
(Dispersion parameter for binomial family taken to be 1)
```

## Проверка на сверхдисперсию

\small

Используем предложенную Беном Болкером функцию проверки на сверхдисперсию,

\vspace{-\baselineskip}


```{r}
# Функция для проверки наличия сверхдисперсии в модели (автор Ben Bolker)
# http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
# Код модифицирован, чтобы учесть дополнительный параметр в NegBin GLMM, подобранных MASS::glm.nb()
overdisp_fun <- function(model) {
    rdf <- df.residual(model)  # Число степеней свободы N - p
    if (any(class(model) == 'negbin')) rdf <- rdf - 1 ## учитываем k в NegBin GLMM
    rp <- residuals(model,type='pearson') # Пирсоновские остатки
    Pearson.chisq <- sum(rp^2) # Сумма квадратов остатков, подчиняется Хи-квадрат распределению 
    prat <- Pearson.chisq/rdf  # Отношение суммы квадратов остатков к числу степеней свободы
    pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE) # Уровень значимости
    c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)        # Вывод результатов
}

overdisp_fun(mod6)
```


Избыточной дисперсии  
не выявлено.


\tiny

Ben Bolker's glmmFAQ  
\url{http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html}


# Визуализация модели




## Данные для предсказаний

```{r}
library(dplyr)
new_data <- astr %>% group_by(Sp)%>%
  do(data.frame(L = seq(min(.$L), max(.$L), length.out = 100)))
```

Давайте получим предсказания при помощи операций с матрицами, чтобы своими глазами увидеть работу функции связи.

## Предсказания модели при помощи операций с матрицами

\small

```{r}
# Модельная матрица и коэффициенты
X <- model.matrix(~ Sp + L, data =  new_data)
b <- coef(mod6)
# Предсказанные значения и стандартные ошибки...
# ...в масштабе функции связи (логит)

new_data$fit_eta <- X %*% b       
new_data$se_eta <- sqrt(diag(X %*% vcov(mod6) %*% t(X)))

# ...в масштабе отклика (применяем функцию, обратную функции связи)

logit_back <- function(x) exp(x)/(1 + exp(x)) # обратная логит-трансформация

new_data$fit_pi <- logit_back(new_data$fit_eta)
new_data$lwr_pi <- logit_back(new_data$fit_eta - 2 * new_data$se_eta)
new_data$upr_pi <- logit_back(new_data$fit_eta + 2 * new_data$se_eta)

head(new_data, 2)
```

## Визуализация в шкале логитов

\columnsbegin
\column{0.48\textwidth}

```{r fig.height=1.4*1.5, out.height='1.4in'}

ggplot(new_data, aes(x = L, y = fit_eta, fill = Sp)) + 
  geom_ribbon(aes(ymin = fit_eta - 2 * se_eta, ymax = fit_eta + 2 * se_eta), alpha = 0.5) +
  geom_line(aes(color = Sp))
```

\column{0.4 \textwidth}

\vspace{6\baselineskip}

Визуализация проведена для логитов, поэтому зависимость линейная, но по оси `OY` отложены значения от $-\infty$ до $+\infty$.

\columnsend


## Визуализация в шкале вероятностей интуитивно понятнее

\columnsbegin
\column{0.48\textwidth}

\small

```{r fig.height=1.4*1.5, out.height='1.4in'}
ggplot(new_data, aes(x = L, y = fit_pi, fill = Sp)) + 
  geom_ribbon(aes(ymin = lwr_pi, ymax = upr_pi), alpha = 0.5) +
  geom_line(aes(color = Sp)) +
  labs(y='Вероятность', title = 'Вероятность быть съеденной')
```

\column{0.48\textwidth}


\columnsend

## О чем говорит модель

Чем больше размер мидии, тем меньше вероятность быть съеденной.

Линия, соответствующая `Tr`, лежит выше линии `Ed`. Вероятность быть атакованной у `Tr` выше.

Значит звезды различают два вида мидий и размер жертвы для них имеет значение.

```{r echo=FALSE, fig.height=1.4*1.5, out.height='1.4in'}
ggplot(new_data, aes(x = L, y = fit_pi, fill = Sp)) + 
  geom_ribbon(aes(ymin = lwr_pi, ymax = upr_pi), alpha = 0.5) +
  geom_line(aes(color = Sp)) +
  labs(y='Вероятность', title = 'Вероятность быть съеденной')
```


## Take-home messages 

### Что мы знаем о бинарных переменных 

- Бинарные переменные-отклики могут обозначаться как угодно (+ или -; Да или Нет). 
- Удобно кодировать бинарные переменные числами: 1 (событие произошло) или 0 (событие не произошло).
- Вместо бинарных обозначений в анализе используются непрерывные оценки вероятности.
- Вероятности можно перевести в отношения шансов.
- Отношения шансов заменяются логитами.

## Take-home messages 

### Что мы знаем о GLM с бинарной переменной-откликом
 
- GLM с бинарной переменной-откликом называют логистической регрессией.   
- Параметры логистической регрессии подбираются методом максимального правдоподобия.
- Угловые коэффициенты логистической регрессии говорят о том, во сколько раз изменяется соотношение шансов для события при увеличении предиктора на единицу (или при переходе от базового уровня фактора к данному уровню).   
- Оценить статистическую значимость модели можно с помощью анализа девиансы.
- Для визуализации результатов лучше проводить обратное логит-преобразование и изображать логистические кривые.


## Что почитать

+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014.
+ Quinn G.P., Keough M.J. (2002) Experimental design and data analysis for biologists, pp. 92-98, 111-130
+ Zuur, A.F. et al. 2009. Mixed effects models and extensions in ecology with R. - Statistics for biology and health. Springer, New York, NY. 

